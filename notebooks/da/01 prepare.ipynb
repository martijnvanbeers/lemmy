{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Training\n",
    "In this notebook, we prepare a dataset which can be used for training a lemmatizer with Lemmy.\n",
    "\n",
    "**NOTE**: You do *not* need to run this notebook to use lemma. The lemmatizer comes trained and ready to use! This notebook is only if you want train the lemmatizer yourself, for example because you want it trained on a specific dataset.\n",
    "\n",
    "We use two datasets which are both publicly available. The first dataset is the word list from Dansk Sprognævn (DSN). This dataset is freely available but you have to sign a contract with DSN to obtain the file. Please see [www.dsn.dk](https://www.dsn.dk) for more info. The other dataset is the Danish part of the Universal Dependencies (UD). This dataset is open source and available from the [UD repo](https://github.com/UniversalDependencies/UD_Danish) on GitHub.\n",
    "\n",
    "The notebook assumes you have the datasets stored in a subfolder called *data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n",
    "import unicodecsv as csv\n",
    "from tqdm import tqdm\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(levelname)s : %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "UD_TRAIN_FILE = \"./data/UD_Danish/da-ud-train.conllu\"\n",
    "DSN_XML_FILE = \"./data/DSN/RO.iLexdump.m.fuldformer.til.aftagere.xml\"\n",
    "PREPARED_FILE = \"./data/prepared.csv\"\n",
    "NORMS_FILE = \"./data/norms.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse DSN XML data\n",
    "Our first step reading the DSN data. We train the lemmatizer to use POS tags to help predict the lemma. We use the UD set of POS tags. Because the word classes used in DSN data differ from UD POS tags, we need to do some manual mapping. The `CLASS_LOOKUP` dictionary specifies the mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_LOOKUP = {\"sb\": [\"NOUN\"],\n",
    "                \"adj\": [\"ADJ\"],\n",
    "                \"adv\": [\"ADV\"],\n",
    "                \"vb\": [\"VERB\"],\n",
    "                \"proprium\": [\"PROPN\"],\n",
    "                \"præp\": [\"ADP\"],\n",
    "                \"udråbsord\": [\"INTJ\"],\n",
    "                \"pron\": [\"PRON\"],\n",
    "                \"talord\": [\"NUM\"],\n",
    "                \"konj\": [\"CONJ\"],\n",
    "                \"romertal\": [\"NUM\"],\n",
    "                \"kolon\": [\"NOUN\"],\n",
    "                \"lydord\": [\"NOUN\"],\n",
    "                \"art\": [\"PRON_DONT_USE\"]}\n",
    "\n",
    "def _build_dsn_tuples(soup):\n",
    "    unknown_classes = defaultdict(int)\n",
    "    forms = set()\n",
    "    homograph_groups = soup.find_all('hom', recursive=True)\n",
    "    for hom_group in tqdm(homograph_groups):\n",
    "        for article in hom_group.find_all(recursive=False):\n",
    "            word_class_temp = article.name.split('-')[0]\n",
    "            word_classes = CLASS_LOOKUP.get(word_class_temp, None)\n",
    "\n",
    "            if not word_classes:\n",
    "                unknown_classes[word_class_temp] += 1\n",
    "                continue\n",
    "\n",
    "            head_node = article.find('hoved')\n",
    "            lemma = head_node.find('opslagsord').get_text()\n",
    "            full_forms = article.find('fuldformer')\n",
    "            if full_forms is None:\n",
    "                continue\n",
    "\n",
    "            if head_node.find('form.af'):\n",
    "                # The lookup word ('artikel') itself is not the baseform, so it will be skipped.\n",
    "                 continue\n",
    "\n",
    "            for full_form_tag in full_forms.find_all('ff', recursive=False):\n",
    "                full_form = full_form_tag.get_text()\n",
    "                for word_class in word_classes:\n",
    "                    forms.add((word_class, full_form, lemma))\n",
    "    return sorted(forms, key = lambda x: x[1:]), unknown_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the XML and parse it using Beautiful Soup\n",
    "soup = BeautifulSoup(open(DSN_XML_FILE), 'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63187/63187 [00:18<00:00, 3407.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build tuples of POS, + full form* and *lemma*\n",
    "dsn_tuples, unknown = _build_dsn_tuples(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse UD data\n",
    "The next step is to read the UD data. We want to learn from both the DSN and UD data. While DSN is the authoritative source, UD does contain words and forms not found in DSN. In case of inconsistencies between DSN and UD, we choose DSN over UD.\n",
    "\n",
    "Some of the UD POS tags, such as *DET* and *AUX*, can not be mapped 1-to-1 to the DSN word classes. Consequently, we learn the words with those POS tags from UD.\n",
    "\n",
    "For adjectives (*ADJ*), the DSN word lists are incomplete. They do not contain various *degrees* for the adjectives, for example the forms *hurtigere* (faster) and *hurtigst* (fastest).\n",
    "\n",
    "UD contains a large amount of proper nouns (*PROPN*) not found in DSN, specifically personal names. We might as well learn from these too, so we read the entire UD training file.\n",
    "\n",
    "Since the UD data is not just a word list but actual sentences annotated with lemmas and POS tags (and more), we have the benefit of having not only the POS tag of the word we want to lemmatize, but also the POS tag of the previous word. We can use this to improve the accuracy of our lemmatizer, so when building the list of tuples from UD, we include the POS tag of the previous word of the sentence. This is set to the empty string when the current word is the first word of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_ud_line(line):\n",
    "    return line.split(\"\\t\")[1:4]\n",
    "\n",
    "def _build_ud_tuples(ud_file, min_freq=1):\n",
    "    counts = {}\n",
    "    pos_prev = \"\"\n",
    "    for line in open(ud_file).readlines():\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "        if line.strip() == \"\":\n",
    "            pos_prev = \"\"\n",
    "            continue\n",
    "\n",
    "        orth, lemma, pos = _parse_ud_line(line)\n",
    "        orth = orth.lower()\n",
    "        lemma = lemma.lower()\n",
    "        key = (pos_prev, pos, orth, lemma)\n",
    "        counts[key] = counts.get(key, 0) + 1\n",
    "        pos_prev = pos\n",
    "    \n",
    "    return [key for key in counts if counts[key] >= min_freq]\n",
    "\n",
    "ud_tuples = _build_ud_tuples(UD_TRAIN_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 'ADP', 'på', 'på'),\n",
       " ('ADP', 'NOUN', 'fredag', 'fredag'),\n",
       " ('NOUN', 'AUX', 'har', 'have'),\n",
       " ('AUX', 'PROPN', 'sid', 'sid'),\n",
       " ('PROPN', 'VERB', 'inviteret', 'invitere')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ud_tuples[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter UD data\n",
    "We will now filter the word forms read from UD. We do this to avoid introducing ambiguity due to spelling errors and typos in UD.\n",
    "\n",
    "We want to include the following only:\n",
    "1. Any POS + full form combination *not* found in DSN.\n",
    "2. Any POS_PREV + POS + full form combination for which the POS + full form is *ambiguous* in DSN + Step 1.\n",
    "\n",
    "By *ambiguous* we mean full forms (or combinations of POS tags and full forms) which have more than one lemma associated with them, which cause the lemmatizer to not know which of the lemmas to choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set for looking up POS + full form combinations found in DSN.\n",
    "dsn_full_forms = set((pos, full_form) for pos, full_form, _lemma in dsn_tuples)\n",
    "\n",
    "# Create a list of POS + full form + lemma tuples from UD for wich the POS + full form combination\n",
    "# is *not* found in DSN.\n",
    "ud_tuples_unique = [(pos, full_form, lemma) for (_pos_prev, pos, full_form, lemma) in ud_tuples if (pos, full_form) not in dsn_full_forms]\n",
    "\n",
    "# Create a new list of tuples consisting of the ones from DSN and the new ones just found in UD.\n",
    "dsn_ud_no_history = dsn_tuples + list(set(ud_tuples_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambiguity\n",
    "Several words can be spelled in more than one way. For example, the Danish word for aquarium, \"akvarium\", can also be spelled *akvarie*. This causes ambiguous rules. If we naively added both \"akvarie\" and \"akvarium\" to our lemmatizer and then tried lemmatizing \"akvarier\" (plural form of \"akvarium\"/\"akvarie\"), the lemmatizer would not know whether to return \"akvarium\" or \"akvarie\". It would then return both words and let the user pick the desired one. In some scenarios, this might be the desired behavior. But here, we will try and avoid the ambiguity.\n",
    "\n",
    "We avoid this kind of ambiguity by choosing one spelling over the other. It doesn't matter too much which spelling we choose as long as we are consistent. In general, when we can easily identify the more 'modern' spelling, we favor that one. In the caae of the aquarium, \"akvarie\" has only recently been accepted and so we will choose that.\n",
    "\n",
    "The \"-ium\"/\"-ie\" endings are quite common and so we will  scan the DSN word list to make sure we handle all of them. We do this by grouping the word list by full form. We then look for groups that contain exactly two lemmas with one ending in \"ium\" and the other in \"ie\". From these we keep only the lemma ending in \"ie\". This ensures that we learn to lemmatize, for example, \"akvarier\" unambiguously to \"akvarie\".\n",
    "\n",
    "We are not done yet, though. What should happen if we lemmatize \"akvarium\"? One option is to just return \"akvarium\" since the word is already in its base form. Another option would be to return \"akvarie\" to reflect that we have chosen that form over \"akvarium\". One might argue in favor of the first option by saying that turning \"akvarium\" into *akvarie* is the job of a normalizer and consequently should not be done by the lemmatizer. On the other hand, leaving different lemmas for different spellings of the same words was what we wanted to avoid. We have chosen to go with option two, lemmatizing \"akvarium\" to \"akvarie\".\n",
    "\n",
    "Note that the disambiguation code is run only during preprocessing of the training (and test) data. If you prefer not to disambiguate in this way, you can skip that part of preprocessing and train your own rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lemma_groups(forms):\n",
    "    groups = {}\n",
    "    for pos, full_form, lemma in forms:\n",
    "        if pos not in groups:\n",
    "            groups[pos] = {}\n",
    "        if full_form not in groups[pos]:\n",
    "            groups[pos][full_form] = []\n",
    "        groups[pos][full_form].append(lemma)\n",
    "    return groups\n",
    "\n",
    "lemma_groups = build_lemma_groups(dsn_ud_no_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('akvarie', ['akvarie']),\n",
       " ('akvarier', ['akvarie', 'akvarium']),\n",
       " ('akvarierne', ['akvarie', 'akvarium']),\n",
       " ('akvariernes', ['akvarie', 'akvarium']),\n",
       " ('akvariers', ['akvarie', 'akvarium']),\n",
       " ('akvaries', ['akvarie']),\n",
       " ('akvariet', ['akvarie', 'akvarium']),\n",
       " ('akvariets', ['akvarie', 'akvarium']),\n",
       " ('akvarium', ['akvarium']),\n",
       " ('akvariums', ['akvarium'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As an example, show ambuigty for various forms of \"akvarie\".\n",
    "[x for x in lemma_groups[\"NOUN\"].items() if x[0].startswith('akvarie') or x[0].startswith('akvariu')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ambiguous(forms_lookup, func_unwanted, func_wanted):\n",
    "    replace_lookup = {}\n",
    "    for full_form in forms_lookup:\n",
    "        lemmas = forms_lookup[full_form]\n",
    "        if len(lemmas) != 2:\n",
    "            continue\n",
    "\n",
    "        unwanted = next((lemma for lemma in lemmas if func_unwanted(lemma)), None)\n",
    "        wanted = next((lemma for lemma in lemmas if func_wanted(lemma)), None)\n",
    "\n",
    "        if not unwanted or not wanted:\n",
    "            continue\n",
    "\n",
    "        forms_lookup[full_form] = [wanted]\n",
    "        replace_lookup[unwanted] = wanted\n",
    "\n",
    "    for full_form in forms_lookup:\n",
    "        lemmas = forms_lookup[full_form]\n",
    "        if len(lemmas) != 1 or lemmas[0] not in replace_lookup:\n",
    "            continue\n",
    "        \n",
    "        unwanted = lemmas[0]\n",
    "        wanted = replace_lookup[unwanted]        \n",
    "        forms_lookup[full_form] = [wanted]\n",
    "    return list(replace_lookup.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2675 nouns with exactly two spellings.\n",
      "Found 2063 nouns with exactly two spellings (after disambiguating ium/ie).\n",
      "Found 2039 nouns with exactly two spellings (after disambiguating fader/far).\n",
      "Found 2007 nouns with exactly two spellings (after disambiguating moder/mor).\n",
      "Found 2003 nouns with exactly two spellings (after disambiguating broder/bror).\n",
      "Found 1995 nouns with exactly two spellings (after disambiguating skifer/skiffer).\n",
      "Found 1967 nouns with exactly two spellings (after disambiguating brille/briller).\n",
      "Found 1959 nouns with exactly two spellings (after disambiguating kon/kum).\n",
      "Found 1953 nouns with exactly two spellings (after disambiguating kjoleskød/kjoleskøde).\n",
      "Found 1941 nouns with exactly two spellings (after disambiguating has/hase).\n",
      "Found 1937 nouns with exactly two spellings (after disambiguating kreol/kreoler).\n",
      "Found 1929 nouns with exactly two spellings (after disambiguating dolmer/dolme).\n",
      "Found 1915 nouns with exactly two spellings (after disambiguating skifte/skift).\n",
      "Found 1909 nouns with exactly two spellings (after disambiguating lomvi/lomvie).\n",
      "Found 1893 nouns with exactly two spellings (after disambiguating blænder/blænde).\n",
      "Found 1887 nouns with exactly two spellings (after disambiguating morlil/morlille).\n",
      "Found 1881 nouns with exactly two spellings (after disambiguating mukkebikke/mukkebik).\n",
      "Found 1877 nouns with exactly two spellings (after disambiguating norman/normanner).\n",
      "Found 1871 nouns with exactly two spellings (after disambiguating padderok/padderokke).\n",
      "Found 1869 nouns with exactly two spellings (after disambiguating padle/paddel).\n",
      "Found 1864 nouns with exactly two spellings (after disambiguating plusfours/plusfour).\n",
      "Found 1860 nouns with exactly two spellings (after disambiguating samaritan/samaritaner).\n",
      "Found 1848 nouns with exactly two spellings (after disambiguating rio/rie).\n",
      "Found 1842 nouns with exactly two spellings (after disambiguating sjægte/sjægt).\n",
      "Found 1834 nouns with exactly two spellings (after disambiguating spektrum/spekter).\n",
      "Found 1828 nouns with exactly two spellings (after disambiguating pse/psis).\n",
      "Found 1824 nouns with exactly two spellings (after disambiguating tandstikker/tandstik).\n",
      "Found 1822 nouns with exactly two spellings (after disambiguating tidsspild/tidsspilde).\n",
      "Found 1818 nouns with exactly two spellings (after disambiguating tjekker/tjekke).\n",
      "Found 1812 nouns with exactly two spellings (after disambiguating trauma/traume).\n",
      "Found 1800 nouns with exactly two spellings (after disambiguating tusinde/tusind).\n",
      "Found 1790 nouns with exactly two spellings (after disambiguating hundrede/hundred).\n",
      "Found 1786 nouns with exactly two spellings (after disambiguating tyrk/tyrker).\n",
      "Found 1782 nouns with exactly two spellings (after disambiguating tøndestave/tøndestav).\n",
      "Found 1776 nouns with exactly two spellings (after disambiguating vable/vabel).\n",
      "Found 1774 nouns with exactly two spellings (after disambiguating unikum/unika).\n",
      "Found 1772 nouns with exactly two spellings (after disambiguating valeriane/valerian).\n",
      "Found 1768 nouns with exactly two spellings (after disambiguating varyler/varyl).\n",
      "Found 1764 nouns with exactly two spellings (after disambiguating baryler/baryl).\n",
      "Found 1746 nouns with exactly two spellings (after disambiguating centrum/center).\n",
      "Found 1714 nouns with exactly two spellings (after disambiguating bo/boer).\n",
      "Found 1690 nouns with exactly two spellings (after disambiguating barne/børne).\n",
      "Found 1686 nouns with exactly two spellings (after disambiguating broder/brødre).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "400699"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run trough various endings and specific words which we know have two accepted spellings and remove one of them.\n",
    "norm_pairs = []\n",
    "t = [(key, value) for (key, value) in lemma_groups[\"NOUN\"].items() if len(value) == 2]\n",
    "print(f\"Found {len(t)} nouns with exactly two spellings.\")\n",
    "for unwanted, wanted in [('ium', 'ie'), ('fader', 'far'), ('moder', 'mor'),\n",
    "                         ('broder', 'bror'), ('skifer', 'skiffer'), ('brille', 'briller'),\n",
    "                         ('kon', 'kum'), ('kjoleskød', 'kjoleskøde'), ('has', 'hase'),\n",
    "                         ('kreol', 'kreoler'), ('dolmer', 'dolme'), ('skifte', 'skift'),\n",
    "                         ('lomvi', 'lomvie'), ('blænder', 'blænde'), ('morlil', 'morlille'),\n",
    "                         ('mukkebikke', 'mukkebik'), ('norman', 'normanner'),\n",
    "                         ('padderok', 'padderokke'), ('padle', 'paddel'),\n",
    "                         ('plusfours', 'plusfour'), ('samaritan', 'samaritaner'),\n",
    "                         ('rio', 'rie'), ('sjægte', 'sjægt'), ('spektrum', 'spekter'),\n",
    "                         ('pse', 'psis'), ('tandstikker', 'tandstik'),\n",
    "                         ('tidsspild', 'tidsspilde'), ('tjekker', 'tjekke'),\n",
    "                         ('trauma', 'traume'), ('tusinde', 'tusind'),\n",
    "                         ('hundrede', 'hundred'), ('tyrk', 'tyrker'),\n",
    "                         ('tøndestave', 'tøndestav'), ('vable', 'vabel'),\n",
    "                         ('unikum', 'unika'), ('valeriane', 'valerian'),\n",
    "                         ('varyler', 'varyl'), ('baryler', 'baryl')]:\n",
    "    norm_pairs += remove_ambiguous(lemma_groups[\"NOUN\"], func_unwanted=lambda x: x.endswith(unwanted), func_wanted=lambda x: x.endswith(wanted))\n",
    "    t = [(key, value) for (key, value) in lemma_groups[\"NOUN\"].items() if len(value) == 2]\n",
    "    print(f\"Found {len(t)} nouns with exactly two spellings (after disambiguating {unwanted}/{wanted}).\")\n",
    "\n",
    "# We want to distinguish between the words \"center\" and \"centrum\" but we want to replace \"centrum\" \n",
    "# with \"center\" whenever it's used as the suffix in a compound word.\n",
    "for unwanted, wanted in [('centrum', 'center'), ('bo', 'boer')]:\n",
    "    t = [(key, value) for (key, value) in lemma_groups[\"NOUN\"].items() if len(value) == 2]\n",
    "    norm_pairs += remove_ambiguous(lemma_groups[\"NOUN\"], func_unwanted=lambda x: x.endswith(unwanted) and len(x) > len(unwanted), func_wanted=lambda x: x.endswith(wanted) and len(x) > len(wanted))\n",
    "    t = [(key, value) for (key, value) in lemma_groups[\"NOUN\"].items() if len(value) == 2]\n",
    "    print(f\"Found {len(t)} nouns with exactly two spellings (after disambiguating {unwanted}/{wanted}).\")\n",
    "\n",
    "for unwanted, wanted in [('barne', 'børne'), ('broder', 'brødre')]:\n",
    "    t = [(key, value) for (key, value) in lemma_groups[\"NOUN\"].items() if len(value) == 2]\n",
    "    norm_pairs += remove_ambiguous(lemma_groups[\"NOUN\"], func_unwanted=lambda x: x.startswith(unwanted), func_wanted=lambda x: x.startswith(wanted))\n",
    "    t = [(key, value) for (key, value) in lemma_groups[\"NOUN\"].items() if len(value) == 2]\n",
    "    print(f\"Found {len(t)} nouns with exactly two spellings (after disambiguating {unwanted}/{wanted}).\")\n",
    "    \n",
    "# Lemma of Danish word \"det\" when POS=='DET' should be \"den\". But UD contains the fixed phrase \"i det hele taget\" in which\n",
    "# the lemma for \"det\" is specified as \"det\". To work around this, we manually disambiguate.\n",
    "lemma_groups[\"DET\"][\"det\"] = [\"den\"]\n",
    "\n",
    "# Finally, now that we have removed ambiguous full forms from the groups, create a new\n",
    "# list of tuples base on the cleaned up groups.\n",
    "clean_dsn_ud_no_history = []\n",
    "for pos in lemma_groups:\n",
    "    for full_form in lemma_groups[pos]:\n",
    "        for lemma in lemma_groups[pos][full_form]:\n",
    "            clean_dsn_ud_no_history.append((pos, full_form, lemma))\n",
    "len(clean_dsn_ud_no_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Ambiguity\n",
    "We have now removed words with two accepted spellings. Unfortunately, we have at least one more kind of ambiguity left in the data, namely distinct words which share one or more forms. For example, the Danish word \"se\" means *see*. Past tense of \"se\" is \"så\" (somewhat similar to *saw*). But the word \"så\" also has another meaning in Danish, namely *sow*. Consequently, if we are to lemmatize the word \"så\" and do not have any other information, we cannot tell whether the lemma is \"se\" or \"så\". For these situation, it helps if we know the POS tag of the previous word of the sentence. Therefor, we now identify ambiguous words which are still present after the above cleaning of ambiguous words. For these ambiguous words, we then build a list of tuples which include the POS tag of the previous word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "424"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_ambiguous_lemmas(forms):\n",
    "    counter = Counter(t[:2] for t in forms)\n",
    "    ambiguous = list(set([key for key in counter if counter[key] > 1]))\n",
    "    return ambiguous\n",
    "\n",
    "ambiguous = find_ambiguous_lemmas(clean_dsn_ud_no_history)\n",
    "dsn_ud_with_history = [(f'{f[0]}_{f[1]}',) + f[2:] for f in ud_tuples if f[1:3] in ambiguous]\n",
    "len(dsn_ud_with_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Tuples To Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _write_form(word_class, full_form, lemma):\n",
    "    writer.writerow([word_class, full_form, lemma])\n",
    "\n",
    "with open(PREPARED_FILE, 'wb') as csvfile:\n",
    "    writer = csv.writer(csvfile,\n",
    "                        delimiter=\",\",\n",
    "                        quotechar='\"',\n",
    "                        quoting=csv.QUOTE_MINIMAL,\n",
    "                        encoding='utf-8',\n",
    "                        lineterminator='\\n')\n",
    "    \n",
    "    writer.writerow(['word_class', 'full_form', 'lemma'])\n",
    "    \n",
    "    for pos, full_form, lemma in sorted(clean_dsn_ud_no_history, key = lambda x: (x[1:], x[0])):\n",
    "        _write_form(pos, full_form, lemma)\n",
    "    for pos, full_form, lemma in sorted(dsn_ud_with_history, key = lambda x: (x[1:], x[0])):\n",
    "        _write_form(pos, full_form, lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Normalizations To Disk\n",
    "At this point, the `norm_pairs` variable contains words we have learned have multiple spellings and for which we have decided to use only one of those spellings. We wrote this list to a CSV file. It will be used one measuring accuracy in another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(norm_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(NORMS_FILE, 'wb') as csvfile:\n",
    "    writer = csv.writer(csvfile,\n",
    "                        delimiter=\",\",\n",
    "                        quotechar='\"',\n",
    "                        quoting=csv.QUOTE_MINIMAL,\n",
    "                        encoding='utf-8',\n",
    "                        lineterminator='\\n')\n",
    "    \n",
    "    writer.writerow(['full_form', 'norm'])\n",
    "    for full_form, norm in norm_pairs:\n",
    "        writer.writerow([full_form, norm])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unused classes from DSN\n",
    "Finally, we will list word classes found in DSN for which we do not have a mapping to UD POS tags. Further investigation for these is an area for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fork 361\n",
      "flerord.forb. 196\n",
      "præfiks 58\n",
      "formelt 2\n"
     ]
    }
   ],
   "source": [
    "for key, value in unknown.items():\n",
    "    print(key, value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
