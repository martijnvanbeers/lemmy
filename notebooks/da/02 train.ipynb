{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a lemmatizer with Lemmy\n",
    "In this notebook, you will see how to train a lemmatizer using lemmy. It assumes you already have a CSV file of the \n",
    "format *pos*, *full_form*, *lemma*. The previous notebook, *01 prepare*, explains how to create such a file using data from Dansk Sprognævn (DSN) and the Universal Dependency (UD) data.\n",
    "\n",
    "We initially create a train/test split and train on the training data only and then evaluate on the train and test set respectively. We then train again on the entire dataset and save the trained rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "from pprint import pformat\n",
    "import pandas as pd\n",
    "from lemmy import Lemmatizer\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(levelname)s : %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPARED_FILE = \"./data/prepared.csv\"\n",
    "TRAINED_RULES_FILE = \"./data/rules.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_examples(lemmatizer):\n",
    "    examples = [[\"VERB\", \"drak\"], [\"NOUN\", \"kattene\"], [\"NOUN\", \"ukrudtet\"], [\"NOUN\", \"slaraffenlandet\"],\n",
    "                [\"NOUN\", \"alen\"], [\"NOUN\", \"skaber\"], [\"NOUN\", \"venskaber\"], [\"NOUN\", \"tilbageførelser\"],\n",
    "                [\"NOUN\", \"aftenbønnerne\"], [\"NOUN\", \"altankassepassere\"]]\n",
    "    for word_class, full_form in examples:\n",
    "        lemma = lemmatizer.lemmatize(word_class, full_form)\n",
    "        print(\"(%s, %s) -> %s\" % (word_class, full_form, lemma))\n",
    "\n",
    "def calculate_accuracy(lemmatizer, X, y):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    ambiguous = 0\n",
    "\n",
    "    for index in range(len(y)):\n",
    "        word_class, full_form = X[index]\n",
    "        target = y[index]\n",
    "        predicted = lemmatizer.lemmatize(word_class, full_form)\n",
    "        total += 1\n",
    "        if len(predicted) > 1:\n",
    "            ambiguous += 1\n",
    "        elif predicted[0] == target:\n",
    "            correct += 1\n",
    "\n",
    "\n",
    "    print(\"correct:\", correct)\n",
    "    print(\"ambiguous:\", ambiguous)\n",
    "    print(\"total:\", total)\n",
    "    print(\"accuracy:\", correct/total)\n",
    "    print(\"ambiguous%:\", ambiguous/total)\n",
    "    print(\"ambiguous + accuracy:\", (ambiguous+correct)/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    df = pd.read_csv(filename, usecols=[0, 1, 2], keep_default_na=False)\n",
    "    df = df.sample(frac=1, random_state=42) # shuffle rows\n",
    "    X = [(word_class, full_form) for _, (word_class, full_form, _) in df.iterrows()]\n",
    "    y = [lemma for _, (_word_class, _full_form, lemma,) in df.iterrows()]\n",
    "    return X, y\n",
    "\n",
    "X, y = load_data(PREPARED_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y):\n",
    "    mask = [False] * len(y)\n",
    "    test_indices = random.sample(range(len(y)), len(y) // 500)\n",
    "    for index in test_indices:\n",
    "        mask[index] = True\n",
    "\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for index, test in enumerate(mask):\n",
    "        if test:\n",
    "            X_test += [X[index]]\n",
    "            y_test += [y[index]]\n",
    "        else:\n",
    "            X_train += [X[index]]\n",
    "            y_train += [y[index]]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "random.seed(42)\n",
    "X_train, y_train, X_test, y_test = split_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete set:     401123\n",
      "Train set:        400321\n",
      "Test set:            802\n"
     ]
    }
   ],
   "source": [
    "print(f\"Complete set: {len(X):10}\")\n",
    "print(f\"Train set:    {len(X_train):10}\")\n",
    "print(f\"Test set:     {len(X_test):10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train temmatizer - training set only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : epoch #1: 45859 rules (45859 new) in 1.83s\n",
      "DEBUG : epoch #2: 60341 rules (14482 new) in 1.63s\n",
      "DEBUG : epoch #3: 62571 rules (2230 new) in 1.60s\n",
      "DEBUG : epoch #4: 63046 rules (475 new) in 1.70s\n",
      "DEBUG : epoch #5: 63168 rules (122 new) in 1.58s\n",
      "DEBUG : epoch #6: 63201 rules (33 new) in 1.52s\n",
      "DEBUG : epoch #7: 63209 rules (8 new) in 1.57s\n",
      "DEBUG : epoch #8: 63209 rules (0 new) in 1.54s\n",
      "DEBUG : training complete: 63209 rules in 13.05s\n",
      "DEBUG : rules before pruning: 63209\n",
      "DEBUG : used rules: 58398\n",
      "DEBUG : rules after pruning: 58398 (4811 removed)\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = Lemmatizer()\n",
    "lemmatizer.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 395843\n",
      "ambiguous: 4478\n",
      "total: 400321\n",
      "accuracy: 0.9888139767836311\n",
      "ambiguous%: 0.011186023216368864\n",
      "ambiguous + accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracy(lemmatizer, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 735\n",
      "ambiguous: 12\n",
      "total: 802\n",
      "accuracy: 0.9164588528678305\n",
      "ambiguous%: 0.014962593516209476\n",
      "ambiguous + accuracy: 0.9314214463840399\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracy(lemmatizer, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(VERB, drak) -> ['drikke']\n",
      "(NOUN, kattene) -> ['kat']\n",
      "(NOUN, ukrudtet) -> ['ukrudt']\n",
      "(NOUN, slaraffenlandet) -> ['slaraffenland']\n",
      "(NOUN, alen) -> ['ale', 'alen', 'al']\n",
      "(NOUN, skaber) -> ['skaber']\n",
      "(NOUN, venskaber) -> ['venskab']\n",
      "(NOUN, tilbageførelser) -> ['tilbageførelse']\n",
      "(NOUN, aftenbønnerne) -> ['aftenbøn']\n",
      "(NOUN, altankassepassere) -> ['altankassepasser']\n"
     ]
    }
   ],
   "source": [
    "print_examples(lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train temmatizer - full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : epoch #1: 45946 rules (45946 new) in 1.70s\n",
      "DEBUG : epoch #2: 60461 rules (14515 new) in 1.66s\n",
      "DEBUG : epoch #3: 62695 rules (2234 new) in 1.54s\n",
      "DEBUG : epoch #4: 63172 rules (477 new) in 1.70s\n",
      "DEBUG : epoch #5: 63294 rules (122 new) in 1.71s\n",
      "DEBUG : epoch #6: 63327 rules (33 new) in 1.67s\n",
      "DEBUG : epoch #7: 63335 rules (8 new) in 1.68s\n",
      "DEBUG : epoch #8: 63335 rules (0 new) in 1.61s\n",
      "DEBUG : training complete: 63335 rules in 13.37s\n",
      "DEBUG : rules before pruning: 63335\n",
      "DEBUG : used rules: 58513\n",
      "DEBUG : rules after pruning: 58513 (4822 removed)\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = Lemmatizer()\n",
    "lemmatizer.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 396627\n",
      "ambiguous: 4496\n",
      "total: 401123\n",
      "accuracy: 0.98879146795372\n",
      "ambiguous%: 0.011208532046280069\n",
      "ambiguous + accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracy(lemmatizer, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Learned Rules\n",
    "We now save the learend rules to a Python file which can be copied to the lemmatizer source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_dict(lemmatizer):\n",
    "    \"\"\"Convert the internal defaultdict to a standard dict.\"\"\"\n",
    "    temp = {}\n",
    "    for pos, rules_ in lemmatizer.rules.items():\n",
    "        if pos not in temp:\n",
    "            temp[pos] = {}\n",
    "\n",
    "        for full_form_suffix, lemma_suffixes_ in rules_.items():\n",
    "            temp[pos][full_form_suffix] = lemma_suffixes_\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAINED_RULES_FILE, 'w') as file:\n",
    "    file.write(\"# coding: utf8\\n\")\n",
    "    file.write(\"from __future__ import unicode_literals\\n\")\n",
    "    file.write(\"\\n\\n\")\n",
    "    file.write(\"rules = \" + pformat(_to_dict(lemmatizer), width=120))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
