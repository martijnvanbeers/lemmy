{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import regex\n",
    "import unicodecsv as csv\n",
    "import lemmy\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMS_FILE = \"./data/norms.csv\"\n",
    "UD_TRAIN_FILE = \"./data/UD_Danish/da-ud-train.conllu\"\n",
    "UD_DEV_FILE = \"./data/UD_Danish/da-ud-dev.conllu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the normalization rules we build in the first notebook. When evaluating, we apply these to the lemmas specified in UD. Otherwise we would risk, for example, counting \"akvarie\" as the incorrect lemma for \"akvarier\" if UD specified the other spelling, \"akvarium\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_lookup = dict([row for row in csv.reader(open(NORMS_FILE, 'rb'), delimiter=\",\",\n",
    "                        quotechar='\"',\n",
    "                        quoting=csv.QUOTE_MINIMAL,\n",
    "                        encoding='utf-8',\n",
    "                        lineterminator='\\n')][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply a few more normalization rules. This is due to DSN and UD not agreeing on the lemmas for certain words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_dsn_normalization = (('PRON', 'det', 'det', 'den'),\n",
    "                        ('ADJ', 'flere', 'mange', 'flere'),\n",
    "                        ('ADJ', 'mere', 'meget', 'mere'),\n",
    "                        ('ADJ', 'meget', 'meget', 'megen'),\n",
    "                        ('ADJ', 'fleste', 'mange', 'flest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load The Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = lemmy.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_ud_line(line):\n",
    "    return line.split(\"\\t\")[1:4]\n",
    "\n",
    "def _evaluate(ud_file):\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    ambiguous = 0\n",
    "    mistakes = {}\n",
    "    ambiguities = {}\n",
    "    pos_prev = \"\"\n",
    "    for line in open(ud_file).readlines():\n",
    "        if line.startswith(\"#\") or line.strip() == \"\":\n",
    "            pos_prev = \"\"\n",
    "            continue\n",
    "\n",
    "        orth, lemma_expected, pos = _parse_ud_line(line)\n",
    "\n",
    "        if pos == \"NOUN\" and lemma_expected in norm_lookup:\n",
    "            lemma_expected = norm_lookup[lemma_expected]\n",
    "        else:\n",
    "            for pos_, orth_, expected_ud, expected_dsn in ud_dsn_normalization:\n",
    "                if pos != pos_ or orth.lower() != orth_ or lemma_expected != expected_ud:\n",
    "                    continue            \n",
    "                lemma_expected = expected_dsn\n",
    "\n",
    "        lemmas_actual = lemmatizer.lemmatize(pos, orth.lower(), pos_previous=pos_prev)    \n",
    "        lemma_actual = lemmas_actual[0]\n",
    "\n",
    "        if len(lemmas_actual) > 1:\n",
    "            ambiguous += 1\n",
    "            ambiguities[(pos, orth)] = ambiguities.get((pos, orth), 0) + 1\n",
    "        elif lemma_actual.lower() == lemma_expected.lower():\n",
    "            correct += 1\n",
    "        else:\n",
    "            mistakes[(pos, orth, lemma_expected, lemma_actual)] = mistakes.get((pos, orth, lemma_expected, lemma_actual), 0) + 1\n",
    "            incorrect += 1\n",
    "        pos_prev = pos\n",
    "\n",
    "    print(\"* correct:\", correct)\n",
    "    print(\"* incorrect:\", incorrect)\n",
    "    print(\"* ambiguous:\", ambiguous)\n",
    "    print(\"*\", correct/(incorrect+ambiguous+correct))\n",
    "    print(\"*\", (correct+ambiguous)/(incorrect+ambiguous+correct))\n",
    "    \n",
    "    return mistakes, ambiguities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on UD Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* correct: 80220\n",
      "* incorrect: 113\n",
      "* ambiguous: 45\n",
      "* 0.9980342879892508\n",
      "* 0.9985941426758566\n"
     ]
    }
   ],
   "source": [
    "mistakes_train, ambiguities_train = _evaluate(UD_TRAIN_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on UD Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* correct: 10246\n",
      "* incorrect: 82\n",
      "* ambiguous: 4\n",
      "* 0.9916763453348819\n",
      "* 0.9920634920634921\n"
     ]
    }
   ],
   "source": [
    "mistakes_dev, ambiguities_dev =_evaluate(UD_DEV_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('PRON', 'de', 'den', 'de'), 20),\n",
       " (('NOUN', 'g', 'gram', 'g'), 8),\n",
       " (('NOUN', 'aftes', 'aften', 'aftes'), 5),\n",
       " (('PRON', 'De', 'den', 'de'), 5),\n",
       " (('ADJ', 'megen', 'meget', 'megen'), 5),\n",
       " (('DET', 'det', 'det', 'den'), 4),\n",
       " (('NOUN', 'forvejen', 'forvej', 'forvejen'), 4),\n",
       " (('ADJ', 'øverste', 'øvre', 'øverst'), 4),\n",
       " (('VERB', 'bortset', 'bortset', 'bortse'), 3),\n",
       " (('ADJ', 'mest', 'meget', 'mest'), 3)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(mistakes_train.items(), key=lambda x: (-x[1], x[0][1].lower(), x))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambiguities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('ADJ', 'mange'), 10),\n",
       " (('NOUN', 'jorden'), 6),\n",
       " (('ADJ', 'Mange'), 6),\n",
       " (('ADV', 'stort'), 4),\n",
       " (('ADJ', '3.'), 3),\n",
       " (('VERB', 'ses'), 3),\n",
       " (('NOUN', 'Jorden'), 2),\n",
       " (('VERB', 'mindes'), 2),\n",
       " (('VERB', 'skændes'), 2),\n",
       " (('NOUN', 'tanke'), 2)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(ambiguities_train.items(), key=lambda x: (-x[1], x[0][1].lower(), x))[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
